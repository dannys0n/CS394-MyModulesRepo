{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzn8KpgjKcFL"
      },
      "source": [
        "# Gradio Travel Planner\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/dannys0n/CS394-MyModulesRepo/blob/main/notebooks/02/Gradio%20Travel%20Planner.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/dannys0n/CS394-MyModulesRepo/tree/main/notebooks/02/Gradio Travel Planner.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XFnewEgKcFM"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xoSxhPlKcFM"
      },
      "outputs": [],
      "source": [
        "!uv pip install gradio==5.49.1 openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfko3ENdKcFN"
      },
      "source": [
        "## Set the OpenRouter API Key from Colab Secrets + initialize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr4YusR6D2Yl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(\n",
        "    base_url='https://openrouter.ai/api/v1',\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRL7sRfWKcFN"
      },
      "source": [
        "## Define some datastructures for prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose these prompts to steer the models away from asking follow-up questions, assisting the user to structured output after streaming instead. my mileage varied.\n",
        "\n",
        "gpt and mistral were chosen as two opposite ends to test the quality of output and my system prompts. qwen was originally the only small model, but I found it took a while to start generating. my assumption is it's a popular model in OpenRouter and was in a queue"
      ],
      "metadata": {
        "id": "BWjGbydPDfX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Avb1INwKcFO"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# system prompts to pass\n",
        "CHAT_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a travel planning expert AI assistant. \"\n",
        "               \"How can you help the user solve their travel question? \"\n",
        "               \"Do not ask follow-up questions, assume a structured output model will take over right after your completion based on your output.\"\n",
        "}\n",
        "STRUCTURED_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"From your thinking, create an itinerary. Take the following previous output and make it structured in json\"\n",
        "}\n",
        "\n",
        "# models for user to pick in UI\n",
        "AVAILABLE_MODELS = [\n",
        "    \"mistralai/ministral-8b-2512\",\n",
        "    \"qwen/qwen3-14b\",\n",
        "    \"openai/gpt-5.2-chat\",\n",
        "]\n",
        "\n",
        "# data structures for an Itinerary\n",
        "class DayPlan(BaseModel):\n",
        "    day: int\n",
        "    summary: str\n",
        "    activities: List[str]\n",
        "\n",
        "class Itinerary(BaseModel):\n",
        "    destination: str\n",
        "    budget: int\n",
        "    daily_schedule: List[DayPlan]\n",
        "    notes: str\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TvTpcwujng"
      },
      "source": [
        "## Define functions for streaming and structured output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally, chat_with_streaming wouldn't implicitly call other functions, I was just lazy."
      ],
      "metadata": {
        "id": "MNlfyjlKD9d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbdBcNTKulkq"
      },
      "outputs": [],
      "source": [
        "# stream output (prompts for structured output + export)\n",
        "def chat_with_streaming(message, history, model_name):\n",
        "    if not history:\n",
        "        messages = [CHAT_SYSTEM_PROMPT, {\"role\": \"user\", \"content\": message}]\n",
        "    else:\n",
        "        messages = history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    response_text = \"\"\n",
        "\n",
        "    # streaming back response\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            token = chunk.choices[0].delta.content\n",
        "            response_text += token\n",
        "            yield response_text, None, None\n",
        "\n",
        "    # stream finished\n",
        "    # generate structured output\n",
        "    structured = generate_structured_output(response_text, model_name)\n",
        "    # generate export\n",
        "    file_path = export_itinerary(structured)\n",
        "\n",
        "    # final update (chat + JSON + export)\n",
        "    yield response_text, structured, file_path\n",
        "\n",
        "# prompt model for structured output\n",
        "def generate_structured_output(response_text, model_name):\n",
        "    messages = [\n",
        "        STRUCTURED_SYSTEM_PROMPT,\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": response_text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.parse(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        response_format=Itinerary\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOEe6vkKcFO"
      },
      "source": [
        "## Define functions for export structured output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exports pure json from structured output. If I had more time, markdown to pdf would have been my choice of prettying export"
      ],
      "metadata": {
        "id": "f4QUylYDFhyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "def export_itinerary(itinerary_json: str) -> str:\n",
        "    tmp = tempfile.NamedTemporaryFile(\n",
        "        mode=\"w\",\n",
        "        suffix=\".json\",\n",
        "        delete=False,\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    data = json.loads(itinerary_json)\n",
        "    json.dump(data, tmp, indent=2, ensure_ascii=False)\n",
        "    tmp.close()\n",
        "\n",
        "    return tmp.name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCen_ejNgoa8"
      },
      "source": [
        "## Start chat interface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI showing streaming, structured output, and download export"
      ],
      "metadata": {
        "id": "f1ibGU4jGTj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCMtvy5cgp9h"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create chat interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Travel Planner AI\")\n",
        "\n",
        "    # dropdown to select model\n",
        "    model_dropdown = gr.Dropdown(\n",
        "        choices=AVAILABLE_MODELS,\n",
        "        value=AVAILABLE_MODELS[0],\n",
        "        label=\"Model\"\n",
        "    )\n",
        "\n",
        "    # grouping outputs\n",
        "    with gr.Row():\n",
        "          # defining structured output + download\n",
        "          with gr.Column(scale=1):\n",
        "              structured_json = gr.JSON(scale=3,label=\"Structured Output\")\n",
        "\n",
        "              download_file = gr.File(\n",
        "                  label=\"Download Itinerary\",\n",
        "                  file_types=[\".json\"],\n",
        "              )\n",
        "          # overall chat window\n",
        "          with gr.Column(scale=3):\n",
        "              chat = gr.ChatInterface(\n",
        "                  fn=chat_with_streaming, # call function and pass arguments\n",
        "                  additional_inputs=[model_dropdown],\n",
        "                  type=\"messages\",\n",
        "                  additional_outputs=[\n",
        "                      structured_json,\n",
        "                      download_file\n",
        "                  ]\n",
        "              )\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use of AI Tools (Academic Disclosure)\n",
        "\n",
        "AI-assisted tools (ChatGPT-5.2 Auto) were used during the development of this notebook to:\n",
        "- Explore design options for a Gradio-based chat interface\n",
        "- Assist in refining Python code\n",
        "- Debug runtime and UI issues\n",
        "\n",
        "All generated content was reviewed, modified, and validated by the author.\n",
        "The author retains responsibility for the correctness and originality of the final submission."
      ],
      "metadata": {
        "id": "YFgeKz_YHXEM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}