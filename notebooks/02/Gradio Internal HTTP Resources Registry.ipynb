{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzn8KpgjKcFL"
      },
      "source": [
        "# Example Internal Registry Query\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/dannys0n/CS394-MyModulesRepo/blob/main/notebooks/02/Gradio%20Internal%20HTTP%20Resources%20Registry.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/dannys0n/CS394-MyModulesRepo/tree/main/notebooks/02/Gradio%20Internal%20HTTP%20Resources%20Registry.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XFnewEgKcFM"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xoSxhPlKcFM",
        "outputId": "98be90f9-e6c5-46ce-f44d-fde2bcda99dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 128ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install gradio==5.49.1 openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfko3ENdKcFN"
      },
      "source": [
        "## Set the OpenRouter API Key from Colab Secrets + initialize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Cr4YusR6D2Yl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(\n",
        "    base_url='https://openrouter.ai/api/v1',\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRL7sRfWKcFN"
      },
      "source": [
        "## Define some datastructures and registry for prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "0Avb1INwKcFO"
      },
      "outputs": [],
      "source": [
        "INTERNAL_DB_HTTP_REGISTRY = {\n",
        "  \"registry.help\": \"https://registry.help.internal.local.fakelink\",\n",
        "\n",
        "  \"dashboard\": \"https://dashboard.internal.local.fakelink\",\n",
        "  \"dashboard.auth\": \"https://dashboard.internal.local/auth.fakelink\",\n",
        "  \"dashboard.payments\": \"https://dashboard.internal.local/payments.fakelink\",\n",
        "\n",
        "  \"metrics\": \"https://metrics.internal.local.fakelink\",\n",
        "  \"metrics.auth.latency\": \"https://metrics.internal.local/auth/latency.fakelink\",\n",
        "  \"metrics.auth.errors\": \"https://metrics.internal.local/auth/errors.fakelink\",\n",
        "  \"metrics.payments.throughput\": \"https://metrics.internal.local/payments/throughput.fakelink\",\n",
        "\n",
        "  \"api\": \"https://api.internal.local.fakelink\",\n",
        "  \"api.auth\": \"https://api.internal.local/auth.fakelink\",\n",
        "  \"api.auth.login\": \"https://api.internal.local/auth/login.fakelink\",\n",
        "  \"api.auth.refresh\": \"https://api.internal.local/auth/refresh.fakelink\",\n",
        "  \"api.payments\": \"https://api.internal.local/payments.fakelink\",\n",
        "  \"api.payments.charge\": \"https://api.internal.local/payments/charge.fakelink\",\n",
        "  \"api.payments.refund\": \"https://api.internal.local/payments/refund.fakelink\",\n",
        "\n",
        "  \"incident\": \"https://incident.internal.local.fakelink\",\n",
        "  \"incident.auth.outage\": \"https://incident.internal.local/auth-outage.fakelink\",\n",
        "  \"incident.payments.timeout\": \"https://incident.internal.local/payments-timeout.fakelink\",\n",
        "  \"incident.database.degraded\": \"https://incident.internal.local/db-degraded.fakelink\",\n",
        "\n",
        "  \"runbook\": \"https://runbook.internal.local.fakelink\",\n",
        "  \"runbook.auth.latency\": \"https://runbook.internal.local/auth-latency.fakelink\",\n",
        "  \"runbook.auth.token_expiry\": \"https://runbook.internal.local/auth-token-expiry.fakelink\",\n",
        "  \"runbook.payments.retry_storm\": \"https://runbook.internal.local/payments-retry-storm.fakelink\",\n",
        "\n",
        "  \"debug\": \"https://debug.internal.local.fakelink\",\n",
        "  \"debug.auth.session\": \"https://debug.internal.local/auth/session.fakelink\",\n",
        "  \"debug.payments.webhook\": \"https://debug.internal.local/payments/webhook.fakelink\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "R06aMBUGqT8v"
      },
      "outputs": [],
      "source": [
        "# system prompts to pass\n",
        "CHAT_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"Your sole purpose is to act as an internal HTTP resources registry retrieval assistant. \"\n",
        "               \"always start your response with 'USR_REQ:' followed by a concise summary of the user's request, then stop.\"\n",
        "               \"Do not ask follow-up questions, do not output json. do not provide links.\"\n",
        "               \"assume a separate structured output model will take over and query database right after your completion based on your output.\"\n",
        "}\n",
        "STRUCTURED_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"From your thinking, create a database resource to return to the user. return null values if not applicable. \"\n",
        "               \"Take the following previous output and search the following internal HTTP resources registry (internal_registry_name:internal_registry_link) for the best match: \"\n",
        "                + str(INTERNAL_DB_HTTP_REGISTRY)\n",
        "}\n",
        "\n",
        "# models for user to pick in UI\n",
        "AVAILABLE_MODELS = [\n",
        "    \"mistralai/ministral-8b-2512\",\n",
        "    \"qwen/qwen3-14b\",\n",
        "    \"openai/gpt-5.2-chat\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfgGAkBrWwRD"
      },
      "source": [
        "## Define datatypes for structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "I1Q3_OJLVatf"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# data structures for an Itinerary\n",
        "class DB_Details(BaseModel):\n",
        "    header_name: str\n",
        "    short_executive_summary: str\n",
        "    bulleted_short_explanations: List[str]\n",
        "\n",
        "class BD_Resource(BaseModel):\n",
        "    internal_registry_name: str\n",
        "    internal_registry_link: str\n",
        "    details: DB_Details\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOEe6vkKcFO"
      },
      "source": [
        "## Define functions for export structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "example-3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "def export_output(output_json: str) -> str:\n",
        "    tmp = tempfile.NamedTemporaryFile(\n",
        "        mode=\"w\",\n",
        "        suffix=\".json\",\n",
        "        delete=False,\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    data = json.loads(output_json)\n",
        "    json.dump(data, tmp, indent=2, ensure_ascii=False)\n",
        "    tmp.close()\n",
        "\n",
        "    return tmp.name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TvTpcwujng"
      },
      "source": [
        "## Define functions for streaming and structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbdBcNTKulkq"
      },
      "outputs": [],
      "source": [
        "# stream output (prompts for structured output + export)\n",
        "def chat_with_streaming(message, history, model_name):\n",
        "    messages = [CHAT_SYSTEM_PROMPT]\n",
        "\n",
        "    if history:\n",
        "        messages.extend(history)\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    response_text = \"\"\n",
        "\n",
        "    # streaming back response\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            token = chunk.choices[0].delta.content\n",
        "            response_text += token\n",
        "            yield response_text, None, None\n",
        "\n",
        "    # stream finished\n",
        "    # generate structured output\n",
        "    structured = generate_structured_output(response_text, model_name)\n",
        "    # generate export\n",
        "    file_path = export_output(structured)\n",
        "\n",
        "    # final update (chat + JSON + export)\n",
        "    yield response_text, structured, file_path\n",
        "\n",
        "# prompt model for structured output\n",
        "def generate_structured_output(response_text, model_name):\n",
        "    messages = [\n",
        "        STRUCTURED_SYSTEM_PROMPT,\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": response_text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.parse(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        response_format=BD_Resource\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCen_ejNgoa8"
      },
      "source": [
        "## Start chat interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ibGU4jGTj7"
      },
      "source": [
        "UI showing streaming, structured output, and download export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCMtvy5cgp9h"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create chat interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Internal HTTP Resources Registry Retrival AI\")\n",
        "\n",
        "    # dropdown to select model\n",
        "    model_dropdown = gr.Dropdown(\n",
        "        choices=AVAILABLE_MODELS,\n",
        "        value=AVAILABLE_MODELS[0],\n",
        "        label=\"Model\"\n",
        "    )\n",
        "\n",
        "    # grouping outputs\n",
        "    with gr.Row():\n",
        "          # defining structured output + download\n",
        "          with gr.Column(scale=2):\n",
        "              structured_json = gr.JSON(scale=3,label=\"Structured Output\")\n",
        "\n",
        "              download_file = gr.File(\n",
        "                  label=\"Download Resource\",\n",
        "                  file_types=[\".json\"],\n",
        "              )\n",
        "          # overall chat window\n",
        "          with gr.Column(scale=1):\n",
        "              chat = gr.ChatInterface(\n",
        "                  fn=chat_with_streaming, # call function and pass arguments\n",
        "                  additional_inputs=[model_dropdown],\n",
        "                  type=\"messages\",\n",
        "                  additional_outputs=[\n",
        "                      structured_json,\n",
        "                      download_file,\n",
        "                  ]\n",
        "              )\n",
        "\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFgeKz_YHXEM"
      },
      "source": [
        "## Use of AI Tools (Academic Disclosure)\n",
        "\n",
        "AI-assisted tools (ChatGPT-5.2 Auto) were used during the development of this notebook to:\n",
        "- Explore design options for a Gradio-based chat interface\n",
        "- Assist in refining Python code\n",
        "- Debug runtime and UI issues\n",
        "\n",
        "All generated content was reviewed, modified, and validated by the author.\n",
        "The author retains responsibility for the correctness and originality of the final submission."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
